# Server Configuration
PORT=3001
NODE_ENV=development

# Server Implementation Selection
# Set to 'true' to use the new modular server, 'false' for legacy monolithic server
# If not set, defaults to 'true' (new server) in all environments
# Set to 'false' to use the legacy monolithic server
USE_NEW_SERVER=true

# Admin Configuration
# Required for admin endpoints like /api/admin/cleanup
ADMIN_TOKEN=your-secret-admin-token

# CORS Configuration (optional)
# Comma-separated list of allowed origins
# CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# Logging Configuration
# Options: error, warn, info, debug
LOG_LEVEL=info

# Session Configuration
# Maximum participants per session
MAX_PARTICIPANTS_PER_SESSION=100

# Cleanup interval in milliseconds (default: 1 hour)
CLEANUP_INTERVAL=3600000

# Session timeout in milliseconds (default: 24 hours)
SESSION_TIMEOUT=86400000

# ============================================
# Voice Command LLM Service Configuration
# ============================================

# Voice Command Processing Strategy:
# - Pattern matching (default): Fast regex-based matching (~5ms)
# - USE_LOCAL_LLM=true: Pattern matching + Ollama fallback for low confidence (~200-800ms)
# - USE_BAML=true: Pattern matching + BAML type-safe LLM fallback (recommended)
#
# BAML provides:
# - Type-safe LLM outputs with auto-generated types
# - Structured response validation
# - Easy provider switching (Ollama, OpenAI, Claude)
# - Better error handling and retries

# Set to 'true' to use BAML for type-safe LLM voice command parsing
# Recommended for production use
# Requires: npm install @boundaryml/baml && npx baml-cli generate
USE_BAML=false

# Set to 'true' to use raw Ollama LLM (legacy, not recommended if BAML is available)
# Only used if USE_BAML=false
# Requires Ollama installation: https://ollama.com/
# See docs/LOCAL_LLM_SETUP.md for complete setup guide
USE_LOCAL_LLM=false

# Ollama Configuration (used by both USE_LOCAL_LLM and USE_BAML)
# Model to use for voice command processing
# Recommended: phi4 (best accuracy), llama3.2:3b (good balance), gemma2:2b (faster)
OLLAMA_MODEL=phi4

# Ollama server URL (default: http://localhost:11434)
OLLAMA_HOST=http://localhost:11434