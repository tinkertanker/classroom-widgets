# Server Configuration
PORT=3001
NODE_ENV=development

# Server Implementation Selection
# Set to 'true' to use the new modular server, 'false' for legacy monolithic server
# If not set, defaults to 'true' (new server) in all environments
# Set to 'false' to use the legacy monolithic server
USE_NEW_SERVER=true

# Admin Configuration
# Required for admin endpoints like /api/admin/cleanup
ADMIN_TOKEN=your-secret-admin-token

# CORS Configuration (optional)
# Comma-separated list of allowed origins
# CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# Logging Configuration
# Options: error, warn, info, debug
LOG_LEVEL=info

# Session Configuration
# Maximum participants per session
MAX_PARTICIPANTS_PER_SESSION=100

# Cleanup interval in milliseconds (default: 1 hour)
CLEANUP_INTERVAL=3600000

# Session timeout in milliseconds (default: 24 hours)
SESSION_TIMEOUT=86400000

# ============================================
# Voice Command Processing Configuration
# ============================================

# Voice Command Processing Strategy:
# - Pattern matching (default): Fast regex-based matching (~5ms)
# - USE_BAML=true: Pattern matching + BAML type-safe AI fallback (~1-3s)
#
# BAML Benefits:
# - Type-safe LLM outputs with auto-generated types
# - Schema-aligned parsing (SAP) handles incomplete responses
# - Works with any LLM via OpenAI-compatible API
# - Automatic validation and error handling

# Set to 'true' to enable BAML AI fallback for low-confidence commands
# Requires: npm install @boundaryml/baml && npx baml-cli generate
# Requires: Ollama running with qwen2.5:0.5b model (or similar)
USE_BAML=true

# BAML Configuration
# Model: qwen2.5:0.5b (0.5GB, fast, excellent JSON output)
# Endpoint: http://localhost:11434/v1 (OpenAI-compatible)
# Confidence threshold: 80% (BAML activates when pattern matching < 80%)
# Optional fields: feedback, shouldSpeak (graceful handling of incomplete responses)