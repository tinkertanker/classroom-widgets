# Docker Compose for Ollama LLM Service
# This runs Ollama in a separate container for voice command processing
#
# Quick Start:
#   docker-compose -f docker-compose.ollama.yml up -d
#   docker-compose -f docker-compose.ollama.yml exec ollama ollama pull phi4
#
# See docs/OLLAMA_DOCKER_SETUP.md for complete guide

version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: classroom-widgets-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    networks:
      - classroom-widgets

    # Resource limits (adjust based on your hardware)
    deploy:
      resources:
        limits:
          cpus: '4'      # Max 4 CPU cores
          memory: 8G     # Max 8GB RAM
        reservations:
          cpus: '2'      # Reserve 2 CPU cores
          memory: 4G     # Reserve 4GB RAM

    # Health check to ensure Ollama is responding
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  ollama-data:
    driver: local

networks:
  classroom-widgets:
    driver: bridge
